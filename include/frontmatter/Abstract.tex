% CREATED BY DAVID FRISK, 2018
Henry Yang\\
Department of Physics\\
Chalmers University of Technology \setlength{\parskip}{0.5cm}

\thispagestyle{plain}			% Supress header 
\setlength{\parskip}{0pt plus 1.0pt}
\section*{Abstract}
\setlength{\parindent}{2em}
Deep neural networks are powerful machine learning constructs that have achieved state of the art performance in a large array of tasks. A major challenge in machine learning is the problem of assessing the predictive uncertainty of deep learning predictors and an unsolved problem to this day. Coupled with this problem is that deep neural networks tends to make overconfident predictions which can lead to incorrect decision making without them being able to recognize it. 

A number of schemes have been proposed in recent years ranging from using Bayesian methodology and Monte Carlo simulations, to reading neuron states in the upstream layers and letting the neural networks learn to recognize it's own certainty. 

In this thesis, we analyze one of these methods that estimates the predictive uncertainty of deep learning algorithms using a deep ensemble. Using classification of handwritten digits as the reference problem, we demonstrate that these methods are effective at assessing predictive uncertainty when faced with out of distribution inputs, and inputs that are distorted by deformation and noise. We also demonstrate that the distribution of the estimated predictive uncertainty differes alot between correctly and incorrectly classified inputs, indicating that the measure uncertainty with this method can be used to predicit incorrect decision making.

% KEYWORDS (MAXIMUM 10 WORDS)
\vfill
Keywords: Deep Learning, Neural Networks, Ensemble Learning, Machine Learning, Entropy, Predictive Uncertainty, Classification, CNN, Perceptrons.

\newpage				% Create empty back of side
\thispagestyle{empty}
\mbox{}